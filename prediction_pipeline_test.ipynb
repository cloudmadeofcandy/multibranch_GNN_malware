{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import stellargraph as sg\n",
    "import pandas as pd\n",
    "from stellargraph import StellarGraph, StellarDiGraph\n",
    "import networkx as nx\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph import globalvar\n",
    "from stellargraph.mapper import GraphSAGENodeGenerator\n",
    "from stellargraph.layer import GraphSAGE\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_patch_matrix(ini_array, rows_to_be_added):\n",
    "\n",
    "    '''\n",
    "    Khi thêm các nút mới vào ma trận, ta có thể làm như sau:\n",
    "    Từ chối kết nối giữa các nút mới với nhau.\n",
    "    Giữ nguyên các kết nối chéo bằng 1.\n",
    "    '''\n",
    "    \n",
    "    if rows_to_be_added.shape[0] <= 1 or len(rows_to_be_added.shape) <= 1:\n",
    "        diag = rows_to_be_added @ rows_to_be_added.T\n",
    "        rows_to_be_added = rows_to_be_added / diag\n",
    "    else:\n",
    "        diag = np.array([i @ i.T for i in rows_to_be_added])\n",
    "        rows_to_be_added = rows_to_be_added / diag[:, np.newaxis]\n",
    "    \n",
    "\n",
    "    arr = np.vstack((ini_array, rows_to_be_added))\n",
    "    rows, cols = arr.shape\n",
    "    pad_width = ((0, max(rows, cols) - rows), (0, max(rows, cols) - cols))\n",
    "    arr = np.pad(arr, pad_width, mode='constant')\n",
    "    np.fill_diagonal(arr, 1)\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_node_to_X(ini_array:pd.DataFrame, row_to_be_added):\n",
    "\n",
    "    \"\"\"\n",
    "    Thêm đặc trưng của nút vào X\n",
    "\n",
    "    Nhận vào 2 đối số:\n",
    "    ini_array: X của đồ thị\n",
    "    row_to_be_added: đặc trưng của nút mới\n",
    "    \"\"\"\n",
    "\n",
    "    ini_array.loc[len(ini_array.index)] = row_to_be_added\n",
    "    return ini_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matr = pd.read_csv('./input/m1.csv', index_col=None, header=None)\n",
    "adj_matr_m2 = pd.read_csv('./input/m2.csv', index_col=None, header=None)\n",
    "adj_matr_m3 = pd.read_csv('./input/m3.csv', index_col=None, header=None)\n",
    "adj_matr_m4 = pd.read_csv('./input/m4.csv', index_col=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "diminutive = np.genfromtxt('input/diminutive_m1.csv', delimiter=',')\n",
    "diminutive_m2 = np.genfromtxt('input/diminutive_m2.csv', delimiter=',')\n",
    "diminutive_m3 = np.genfromtxt('input/diminutive_m3.csv', delimiter=',')\n",
    "diminutive_m4 = np.genfromtxt('input/diminutive_m4.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('input/app_api.csv', header=0, index_col='Unnamed: 0')\n",
    "X_row, X_col = X.shape\n",
    "labels = X.pop('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is strictly for m1\n",
    "adj_matr = adj_matr.to_numpy()\n",
    "adj_matr = adj_matr / np.amax(adj_matr, axis=1)\n",
    "adj_matr = np.tril(adj_matr)\n",
    "\n",
    "#this is strictly for m2\n",
    "adj_matr_m2 = adj_matr_m2.to_numpy()\n",
    "adj_matr_m2 = adj_matr_m2 / np.amax(adj_matr_m2, axis=1)\n",
    "adj_matr_m2 = np.tril(adj_matr_m2)\n",
    "\n",
    "#this is strictly for m3\n",
    "adj_matr_m3 = adj_matr_m3.to_numpy()\n",
    "adj_matr_m3 = adj_matr_m3 / np.amax(adj_matr_m3, axis=1)\n",
    "adj_matr_m3 = np.tril(adj_matr_m3)\n",
    "\n",
    "#this is strictly for m4\n",
    "adj_matr_m4 = adj_matr_m4.to_numpy()\n",
    "adj_matr_m4 = adj_matr_m4 / np.amax(adj_matr_m4, axis=1)\n",
    "adj_matr_m4 = np.tril(adj_matr_m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('input/app_api_test_1000.csv', header=0, index_col='Unnamed: 0') #X_test là tập dữ liệu chứa các đặc trưng của các nút mới\n",
    "X_test = X_test[200:399]\n",
    "test_labels = X_test.pop('Label')\n",
    "test_row, test_col = X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = np.arange(X_row, X_row + test_row, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = pd.concat([X, X_test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = X_test.to_numpy() @ diminutive\n",
    "A2 = X_test.to_numpy() @ diminutive_m2\n",
    "A3 = X_test.to_numpy() @ diminutive_m3\n",
    "A4 = X_test.to_numpy() @ diminutive_m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matr = l_patch_matrix(adj_matr, A1)\n",
    "adj_matr_m2 = l_patch_matrix(adj_matr_m2, A2)\n",
    "adj_matr_m3 = l_patch_matrix(adj_matr_m3, A3)\n",
    "adj_matr_m4 = l_patch_matrix(adj_matr_m4, A4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 5199, Edges: 13437669\n",
      "\n",
      " Node types:\n",
      "  default: [5199]\n",
      "    Features: float32 vector, length 400\n",
      "    Edge types: default-default->default\n",
      "\n",
      " Edge types:\n",
      "    default-default->default: [13437669]\n",
      "        Weights: range=[5.88301e-09, 1], mean=0.416206, std=0.329732\n",
      "        Features: none\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = nx.Graph(adj_matr)\n",
    "graph_full = sg.StellarGraph.from_networkx(G, node_features=X_full)\n",
    "del G, adj_matr\n",
    "\n",
    "G_m2 = nx.Graph(adj_matr_m2)\n",
    "graph_full_m2 = sg.StellarGraph.from_networkx(G_m2, node_features=X_full)\n",
    "del G_m2, adj_matr_m2\n",
    "\n",
    "G_m3 = nx.Graph(adj_matr_m3)\n",
    "graph_full_m3 = sg.StellarGraph.from_networkx(G_m3, node_features=X_full)\n",
    "del G_m3, adj_matr_m3\n",
    "\n",
    "G_m4 = nx.Graph(adj_matr_m4)\n",
    "graph_full_m4 = sg.StellarGraph.from_networkx(G_m4, node_features=X_full)\n",
    "del G_m4, adj_matr_m4\n",
    "\n",
    "print(graph_full.info())\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(graph_full, open(\"./output/graph_full_m1.pkl\", \"wb\"))\n",
    "# pickle.dump(graph_full_m2, open(\"./output/graph_full_m2.pkl\", \"wb\"))\n",
    "# pickle.dump(graph_full_m3, open(\"./output/graph_full_m3.pkl\", \"wb\"))\n",
    "# pickle.dump(graph_full_m4, open(\"./output/graph_full_m4.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gf1= pickle.load(open(\"./output/graph_full_m1.pkl\", \"rb\"))\n",
    "# print(gf1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010,\n",
       "       5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021,\n",
       "       5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032,\n",
       "       5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043,\n",
       "       5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054,\n",
       "       5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065,\n",
       "       5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076,\n",
       "       5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087,\n",
       "       5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098,\n",
       "       5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109,\n",
       "       5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120,\n",
       "       5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131,\n",
       "       5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142,\n",
       "       5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153,\n",
       "       5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164,\n",
       "       5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175,\n",
       "       5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186,\n",
       "       5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197,\n",
       "       5198])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoding = preprocessing.LabelBinarizer()\n",
    "\n",
    "targets = target_encoding.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_samples = [10, 10]\n",
    "\n",
    "generator = GraphSAGENodeGenerator(graph_full, batch_size, num_samples, weighted=True)\n",
    "gen = generator.flow(test_index, test_labels)\n",
    "\n",
    "generator_m2 = GraphSAGENodeGenerator(graph_full_m2, batch_size, num_samples, weighted=True)\n",
    "gen_m2 = generator_m2.flow(test_index, test_labels)\n",
    "\n",
    "generator_m3 = GraphSAGENodeGenerator(graph_full_m3, batch_size, num_samples, weighted=True)\n",
    "gen_m3 = generator_m3.flow(test_index, test_labels)\n",
    "\n",
    "generator_m4 = GraphSAGENodeGenerator(graph_full_m4, batch_size, num_samples, weighted=True)\n",
    "gen_m4 = generator_m4.flow(test_index, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/nckh/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/root/miniconda3/envs/nckh/lib/python3.7/site-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n",
      "/root/miniconda3/envs/nckh/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/root/miniconda3/envs/nckh/lib/python3.7/site-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/root/miniconda3/envs/nckh/lib/python3.7/site-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/root/miniconda3/envs/nckh/lib/python3.7/site-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if sys.path[0] == \"\":\n",
      "/root/miniconda3/envs/nckh/lib/python3.7/site-packages/ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  app.launch_new_instance()\n",
      "/root/miniconda3/envs/nckh/lib/python3.7/site-packages/ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/root/miniconda3/envs/nckh/lib/python3.7/site-packages/ipykernel_launcher.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/root/miniconda3/envs/nckh/lib/python3.7/site-packages/ipykernel_launcher.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/root/miniconda3/envs/nckh/lib/python3.7/site-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/root/miniconda3/envs/nckh/lib/python3.7/site-packages/ipykernel_launcher.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = np.vstack(np.array([gen[i][0][0] for i in range(len(gen))]))\n",
    "input_2 = np.vstack(np.array([gen[i][0][1] for i in range(len(gen))]))\n",
    "input_3 = np.vstack(np.array([gen[i][0][2] for i in range(len(gen))]))\n",
    "\n",
    "# label = np.vstack([gen[i][1] for i in range(len(gen))])\n",
    "\n",
    "del gen\n",
    "gc.collect()\n",
    "\n",
    "input_4 = np.vstack(np.array([gen_m2[i][0][0] for i in range(len(gen_m2))]))\n",
    "input_5 = np.vstack(np.array([gen_m2[i][0][1] for i in range(len(gen_m2))]))\n",
    "input_6 = np.vstack(np.array([gen_m2[i][0][2] for i in range(len(gen_m2))]))\n",
    "\n",
    "del gen_m2\n",
    "gc.collect()\n",
    "\n",
    "input_7 = np.vstack(np.array([gen_m3[i][0][0] for i in range(len(gen_m3))]))\n",
    "input_8 = np.vstack(np.array([gen_m3[i][0][1] for i in range(len(gen_m3))]))\n",
    "input_9 = np.vstack(np.array([gen_m3[i][0][2] for i in range(len(gen_m3))]))\n",
    "\n",
    "del gen_m3\n",
    "gc.collect()\n",
    "\n",
    "input_10 = np.vstack(np.array([gen_m4[i][0][0] for i in range(len(gen_m4))]))\n",
    "input_11 = np.vstack(np.array([gen_m4[i][0][1] for i in range(len(gen_m4))]))\n",
    "input_12 = np.vstack(np.array([gen_m4[i][0][2] for i in range(len(gen_m4))]))\n",
    "\n",
    "del gen_m4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/nckh/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  f\"The initializer {self.__class__.__name__} is unseeded \"\n"
     ]
    }
   ],
   "source": [
    "graphsage_model = GraphSAGE(layer_sizes=[64, 32], generator=generator, bias=True, dropout=0.5,)\n",
    "\n",
    "graphsage_model_m2 = GraphSAGE(layer_sizes=[64, 32], generator=generator_m2, bias=True, dropout=0.5,)\n",
    "\n",
    "graphsage_model_m3 = GraphSAGE(layer_sizes=[64, 32], generator=generator_m3, bias=True, dropout=0.5,)\n",
    "\n",
    "graphsage_model_m4 = GraphSAGE(layer_sizes=[64, 32], generator=generator_m4, bias=True, dropout=0.5,)\n",
    "\n",
    "x_inp, x_out = graphsage_model.in_out_tensors()\n",
    "prediction = layers.Dense(units=targets.shape[1], activation=\"softmax\")(x_out)\n",
    "\n",
    "x_inp_m2, x_out_m2 = graphsage_model_m2.in_out_tensors()\n",
    "prediction_m2 = layers.Dense(units=targets.shape[1], activation=\"softmax\")(x_out_m2)\n",
    "\n",
    "x_inp_m3, x_out_m3 = graphsage_model_m3.in_out_tensors()\n",
    "prediction_m3 = layers.Dense(units=targets.shape[1], activation=\"softmax\")(x_out_m3)\n",
    "\n",
    "x_inp_m4, x_out_m4 = graphsage_model_m4.in_out_tensors()\n",
    "prediction_m4 = layers.Dense(units=targets.shape[1], activation=\"softmax\")(x_out_m4)\n",
    "\n",
    "model = Model(inputs=x_inp, outputs=prediction)\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.005), loss=losses.categorical_crossentropy, metrics=[\"acc\"],)\n",
    "\n",
    "model_m2 = Model(inputs=x_inp_m2, outputs=prediction_m2)\n",
    "model_m2.compile( optimizer=optimizers.Adam(learning_rate=0.005), loss=losses.categorical_crossentropy, metrics=[\"acc\"],)\n",
    "\n",
    "model_m3 = Model(inputs=x_inp_m3, outputs=prediction_m3)\n",
    "model_m3.compile( optimizer=optimizers.Adam(learning_rate=0.005), loss=losses.categorical_crossentropy, metrics=[\"acc\"],)\n",
    "\n",
    "model_m4 = Model(inputs=x_inp_m4, outputs=prediction_m4)\n",
    "model_m4.compile(optimizer=optimizers.Adam(learning_rate=0.005),loss=losses.categorical_crossentropy,metrics=[\"acc\"],)\n",
    "\n",
    "combined = layers.concatenate([model.output, model_m2.output, model_m3.output, model_m4.output])\n",
    "\n",
    "z = layers.Dense(units=targets.shape[1], activation=\"softmax\")(combined)\n",
    "\n",
    "combined_model = Model(inputs=[x_inp, x_inp_m2, x_inp_m3, x_inp_m4], outputs=z)\n",
    "\n",
    "\n",
    "combined_model.compile(\n",
    "\n",
    "optimizer=optimizers.Adam(learning_rate=0.005),\n",
    "\n",
    "loss=losses.categorical_crossentropy,\n",
    "\n",
    "metrics=[\"acc\"],\n",
    "\n",
    ")\n",
    "\n",
    "combined_model.load_weights('./input/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "hold_out_predictions = combined_model.predict([input_1, input_2, input_3, input_4, input_5, input_6, input_7, input_8, input_9, input_10, input_11, input_12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_predictions = target_encoding.inverse_transform(hold_out_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2864321608040201\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "a = pd.DataFrame()\n",
    "a.insert(0, \"1\", test_labels)\n",
    "a.insert(1, \"2\", hold_out_predictions)\n",
    "a.to_csv(\"output/predict_result.csv\")\n",
    "\n",
    "print(accuracy_score(test_labels, hold_out_predictions))\n",
    "# print(recall_score(test_labels, hold_out_predictions, average='macro'))\n",
    "# print(f1_score(test_labels, hold_out_predictions, average='macro'))\n",
    "# print(precision_score(test_labels, hold_out_predictions, average='macro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   0,   0,   0,   0],\n",
       "       [  3,  56,   3,  21, 115],\n",
       "       [  0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_labels, hold_out_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
